{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight Parameter containing:\n",
      "tensor([[[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]]], requires_grad=True)\n",
      "conv1.bias Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "conv2.weight Parameter containing:\n",
      "tensor([[[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]]], requires_grad=True)\n",
      "conv2.bias Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       requires_grad=True)\n",
      "fc1.weight Parameter containing:\n",
      "tensor([[ 0.0967, -0.0692, -0.0967,  ...,  0.0193,  0.0537, -0.0951],\n",
      "        [ 0.0230,  0.0489,  0.0233,  ..., -0.0220,  0.0782,  0.0788],\n",
      "        [ 0.0278, -0.1124,  0.0360,  ...,  0.0493,  0.0476,  0.0710],\n",
      "        ...,\n",
      "        [-0.0224,  0.0314, -0.0714,  ...,  0.0251,  0.0739,  0.0692],\n",
      "        [-0.0087, -0.0409, -0.1029,  ..., -0.0053, -0.0817, -0.0160],\n",
      "        [-0.0491, -0.0065,  0.0045,  ...,  0.0839, -0.1429, -0.0639]],\n",
      "       requires_grad=True)\n",
      "fc1.bias Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "fc2.weight Parameter containing:\n",
      "tensor([[ 0.1057, -0.0816, -0.0991,  ..., -0.0613, -0.0594,  0.0559],\n",
      "        [ 0.0250, -0.0259,  0.0380,  ...,  0.0486, -0.0772,  0.0705],\n",
      "        [-0.0681, -0.0020, -0.1356,  ...,  0.0901,  0.0728,  0.0465],\n",
      "        ...,\n",
      "        [ 0.1501,  0.0173, -0.1146,  ...,  0.0003, -0.0463,  0.0582],\n",
      "        [ 0.0470,  0.0290,  0.0337,  ..., -0.0151, -0.0290,  0.0503],\n",
      "        [ 0.1258, -0.1133, -0.0363,  ..., -0.1093, -0.1518, -0.0414]],\n",
      "       requires_grad=True)\n",
      "fc2.bias Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "fc3.weight Parameter containing:\n",
      "tensor([[ 7.4365e-03,  7.4228e-02,  7.0924e-02,  9.6192e-02, -7.8050e-02,\n",
      "          4.3509e-02, -9.5338e-02, -1.4657e-01,  2.0214e-01, -5.6987e-02,\n",
      "         -6.8369e-03, -1.9120e-03, -1.4242e-01, -4.6906e-02,  5.5086e-02,\n",
      "          7.5856e-02,  6.9270e-03, -2.1204e-01,  2.0130e-02, -8.9155e-02,\n",
      "         -5.0072e-03, -3.6481e-02, -3.1040e-02, -1.7205e-01, -3.7274e-02,\n",
      "          5.5234e-02,  1.6098e-01, -1.1631e-01,  5.1233e-02, -1.3739e-01,\n",
      "          4.1291e-03,  3.2401e-02, -1.6795e-01,  2.2366e-01, -1.2367e-01,\n",
      "         -9.4928e-02,  6.6849e-02,  3.4742e-02, -1.0544e-01,  3.3730e-02,\n",
      "          1.1705e-02, -8.0720e-02, -6.2684e-03,  1.1361e-01,  1.6396e-01,\n",
      "          2.7003e-02,  1.1016e-01,  1.3787e-01,  1.2813e-01,  2.6473e-01,\n",
      "         -1.1451e-01,  1.7872e-01,  1.8400e-01,  3.6554e-02, -7.3215e-02,\n",
      "         -1.9042e-02,  2.9115e-03, -9.3835e-03, -1.1767e-01, -4.4472e-03,\n",
      "         -1.9425e-01,  9.2514e-02, -9.0423e-03,  9.1251e-02,  4.5474e-02,\n",
      "          1.2059e-01,  3.0023e-01,  8.5435e-02,  9.1142e-03, -4.7829e-02,\n",
      "         -9.0551e-02,  1.5294e-01, -1.0104e-01, -8.5427e-02, -1.3209e-01,\n",
      "         -4.0045e-02,  2.2466e-02, -8.1349e-02,  9.1371e-02, -2.3189e-01,\n",
      "          7.4061e-02, -1.2135e-01, -2.0789e-02, -4.8616e-03],\n",
      "        [-6.2497e-03, -9.0404e-02, -9.6356e-02, -3.9535e-02, -5.8769e-02,\n",
      "          1.0663e-01,  3.3324e-02, -7.9359e-02, -2.2269e-02,  9.0178e-02,\n",
      "          9.6656e-02, -1.8316e-02, -1.1419e-01, -8.3251e-02, -1.7836e-01,\n",
      "         -8.8086e-02,  9.6566e-02,  3.5170e-02, -9.6116e-02,  1.1442e-01,\n",
      "          3.9078e-01, -1.2481e-01,  1.7000e-01, -6.1670e-04, -1.2463e-02,\n",
      "          2.2766e-02,  7.4623e-02, -6.0570e-02, -2.8699e-03,  7.9176e-03,\n",
      "          2.4225e-01,  8.8614e-02,  4.9136e-02, -3.1906e-02,  3.1343e-02,\n",
      "          9.3427e-02,  1.7769e-01, -4.4846e-02,  4.1438e-02, -1.0467e-01,\n",
      "          1.6412e-01,  9.7869e-02, -1.7445e-02,  1.1949e-01, -2.8647e-02,\n",
      "         -1.5407e-01,  1.7321e-02,  4.1568e-02,  1.5060e-01, -1.3979e-02,\n",
      "         -4.8354e-03,  5.5863e-02,  1.2204e-02, -1.0754e-01, -3.4885e-02,\n",
      "          1.0934e-01,  8.7264e-02,  7.5076e-02, -1.9381e-01,  3.1714e-03,\n",
      "         -7.0281e-02,  1.4298e-02,  1.3819e-01, -2.7838e-02, -1.1394e-01,\n",
      "         -1.4783e-02, -1.5468e-01,  5.4997e-02,  9.3846e-02,  1.4599e-01,\n",
      "          2.0177e-01,  1.0156e-02, -9.4144e-02, -2.8542e-03,  2.3205e-01,\n",
      "          8.9824e-02, -2.1956e-01, -1.2033e-01,  1.4539e-01, -1.7910e-01,\n",
      "          4.0237e-02,  2.8955e-02, -1.0090e-01,  4.3641e-03],\n",
      "        [-1.7185e-01, -1.3545e-01,  1.2437e-02,  7.9913e-02, -7.1959e-02,\n",
      "         -2.4425e-02,  1.6976e-01, -1.1487e-01, -3.8584e-02, -1.8093e-01,\n",
      "         -7.2551e-02,  1.1434e-01, -1.3503e-01, -3.7247e-02,  1.3747e-02,\n",
      "         -6.0765e-02, -1.0893e-01,  4.8855e-02, -8.1149e-02, -1.0640e-01,\n",
      "         -2.9517e-02, -1.8757e-01, -1.1759e-01,  2.1398e-02, -2.2121e-01,\n",
      "         -1.2858e-01,  8.4784e-02,  1.2285e-02,  3.5975e-02,  1.5653e-01,\n",
      "         -1.1923e-01,  6.9827e-02,  1.1270e-01, -1.2022e-01,  2.3514e-03,\n",
      "         -1.6708e-01,  9.7515e-02,  1.0983e-01, -1.6926e-01, -2.3746e-03,\n",
      "          1.3421e-01,  1.6491e-01,  1.3595e-01,  8.1057e-02,  7.7367e-02,\n",
      "         -5.4424e-02,  7.5623e-02, -3.2866e-02,  2.0359e-02,  9.6608e-02,\n",
      "          1.3449e-01,  4.8940e-02, -6.7535e-02,  2.0619e-01, -5.4664e-02,\n",
      "          1.2306e-01, -9.8254e-02, -1.9001e-04, -1.6875e-01, -5.6014e-02,\n",
      "          1.9351e-01,  1.4803e-02,  7.9542e-02,  7.0596e-02,  7.8223e-03,\n",
      "          2.0666e-02, -1.0259e-01, -1.0549e-01, -6.8247e-02, -7.5995e-02,\n",
      "          1.2954e-01,  2.0724e-02,  1.1316e-01,  4.3486e-02, -1.2396e-01,\n",
      "          8.9499e-02,  1.4382e-01,  1.4145e-01,  1.6399e-01, -6.7398e-02,\n",
      "         -9.5635e-02,  4.3830e-02,  2.1622e-01, -1.6353e-01],\n",
      "        [ 1.0091e-01,  7.9629e-02, -2.1546e-01,  6.2727e-02,  2.1806e-01,\n",
      "         -1.1760e-02,  5.9686e-02, -1.0864e-01,  5.1819e-02, -1.5453e-02,\n",
      "          9.7134e-04,  1.3039e-02,  2.1012e-02,  1.6857e-01,  8.3286e-02,\n",
      "         -1.4173e-01, -1.1784e-01,  5.7242e-02, -8.5910e-02,  3.6044e-02,\n",
      "          3.3200e-02, -5.2955e-02, -1.2548e-01,  1.0206e-01, -2.1259e-01,\n",
      "          1.6000e-01, -1.2020e-01, -1.5067e-01, -3.4885e-02,  2.0582e-02,\n",
      "          2.0674e-02, -1.9441e-01,  1.6309e-02,  1.6222e-01,  1.1311e-01,\n",
      "         -2.3188e-01,  9.6414e-02, -8.9851e-02, -3.1680e-02,  1.2730e-01,\n",
      "          8.0052e-03,  1.3644e-01, -6.8647e-02,  8.4093e-02, -7.3343e-02,\n",
      "         -8.7872e-02, -6.8365e-02, -9.8454e-03, -4.4268e-02,  2.1819e-01,\n",
      "          3.3498e-02, -8.1149e-04,  3.4485e-02, -1.7734e-01,  1.5728e-01,\n",
      "         -2.2550e-01,  2.1467e-01, -7.5927e-02, -2.1805e-02,  1.9525e-01,\n",
      "         -1.0252e-01,  3.2481e-02,  1.2501e-01, -4.5869e-02, -8.2930e-02,\n",
      "         -1.6430e-01, -1.2177e-02, -1.6088e-02,  4.6405e-02,  3.5780e-02,\n",
      "          6.8421e-03, -5.4808e-02,  6.4080e-02, -1.9210e-02, -3.1306e-02,\n",
      "         -1.4824e-01,  1.0628e-01, -2.4731e-02, -8.4689e-02, -8.9632e-02,\n",
      "         -1.5644e-01,  1.5478e-01,  7.2006e-02,  6.1123e-02],\n",
      "        [-8.0320e-02, -9.9318e-02,  1.4449e-01, -5.5489e-02,  4.8923e-02,\n",
      "         -3.6152e-02,  1.1761e-01, -3.7744e-02, -1.0298e-01,  8.5047e-02,\n",
      "         -2.6988e-01, -8.0069e-02, -3.6892e-02,  3.8811e-02, -7.5153e-02,\n",
      "          9.0322e-02, -5.5495e-02, -9.2923e-02,  1.0152e-01,  9.8092e-02,\n",
      "          6.9737e-02,  8.1853e-02,  2.1372e-01,  2.7051e-02,  3.9735e-02,\n",
      "         -6.5329e-02,  3.7647e-02,  7.1165e-02, -1.8752e-02, -1.0449e-01,\n",
      "         -8.1849e-02, -1.0229e-01,  3.4281e-02, -9.0975e-02, -7.7562e-02,\n",
      "          1.4836e-02, -3.1795e-01, -7.6815e-02,  2.4670e-02,  1.3226e-01,\n",
      "         -4.4144e-02, -1.5820e-01, -1.6656e-01, -3.5907e-02, -6.5737e-02,\n",
      "         -1.3609e-01, -2.7423e-02, -7.2499e-02,  1.2063e-01,  9.9664e-02,\n",
      "         -1.3900e-02, -1.3690e-01,  9.5958e-02,  4.8504e-02, -1.0005e-02,\n",
      "          2.2974e-01,  1.2377e-01,  1.1900e-01, -2.1988e-02,  1.2340e-01,\n",
      "         -2.6776e-02,  1.7860e-01,  9.3978e-02, -9.5575e-02, -1.1284e-03,\n",
      "         -1.3677e-01,  8.2889e-03, -4.1557e-02,  1.8354e-01,  6.4844e-03,\n",
      "         -9.8353e-03,  6.6186e-02, -1.1708e-02, -9.5009e-02, -2.2210e-01,\n",
      "         -1.9272e-02, -3.4309e-02, -2.5071e-02,  1.6213e-01, -1.0486e-01,\n",
      "         -2.4829e-01,  8.7114e-02,  1.9773e-01,  4.8814e-02],\n",
      "        [ 3.0594e-01,  1.7550e-01,  8.5484e-02,  1.4268e-02, -2.1059e-01,\n",
      "         -8.0958e-02,  1.1361e-01, -1.9851e-01, -6.1061e-02, -7.5880e-02,\n",
      "         -3.0493e-01,  5.7203e-02, -8.4105e-02,  1.8396e-01,  1.0867e-01,\n",
      "          6.1540e-02, -3.4317e-02,  5.4960e-03,  7.9618e-02,  1.6803e-01,\n",
      "         -7.1432e-02,  1.4719e-01,  1.3120e-01,  6.5977e-02, -7.8960e-02,\n",
      "          7.7477e-03, -4.3855e-02,  1.3510e-01, -8.2849e-02,  1.4854e-01,\n",
      "         -1.3739e-01, -6.9290e-02,  9.1462e-02,  2.3920e-01, -1.0908e-01,\n",
      "          9.9768e-02,  1.1636e-01,  2.2251e-02, -1.4140e-01, -2.9551e-02,\n",
      "          1.5462e-01,  1.3284e-01, -1.2295e-02, -1.8437e-01,  2.3894e-02,\n",
      "         -1.1301e-01,  9.7668e-02,  1.0553e-01,  1.0649e-01,  6.9146e-03,\n",
      "          2.9052e-02,  2.1067e-02,  1.1828e-02,  6.5644e-02,  2.2030e-02,\n",
      "          4.2372e-02,  1.0013e-01,  1.1690e-01, -1.5676e-02,  1.0710e-01,\n",
      "          9.1576e-02, -1.5856e-01, -2.5807e-02,  8.5238e-02, -1.6766e-03,\n",
      "          4.5966e-02, -1.3830e-01,  6.2491e-03,  1.5479e-02,  6.7818e-02,\n",
      "         -1.4592e-01,  4.9464e-02, -3.1870e-02,  4.3673e-03,  1.4987e-01,\n",
      "          3.3632e-02, -7.7809e-02, -5.4632e-03, -7.8589e-02,  3.6702e-02,\n",
      "          6.9316e-02, -9.0414e-02, -9.9820e-02,  8.4335e-03],\n",
      "        [ 4.3840e-03, -3.6450e-02, -7.6067e-03, -8.0027e-03, -7.5360e-02,\n",
      "          4.1398e-04,  7.8685e-02,  7.5152e-02,  1.5309e-01,  8.6295e-02,\n",
      "          1.3499e-01, -4.7011e-02, -2.1321e-01,  1.0840e-01, -1.2191e-02,\n",
      "          1.8241e-01, -1.8761e-03, -1.0840e-01,  1.2413e-01,  1.7723e-02,\n",
      "         -6.1288e-02, -7.1822e-02,  4.6810e-02, -1.2765e-01,  1.8973e-01,\n",
      "          1.6936e-01, -2.9025e-02, -1.3838e-01, -2.1795e-02,  4.0724e-02,\n",
      "         -1.7811e-01,  1.3778e-01, -3.5189e-04,  1.1458e-01,  2.2183e-01,\n",
      "         -4.0001e-02, -3.4816e-02, -3.7018e-02,  2.7927e-02, -1.3651e-01,\n",
      "         -8.2359e-02,  2.4067e-01, -6.4268e-02, -8.4444e-02, -1.3364e-02,\n",
      "         -1.6069e-01,  4.5740e-03, -4.5998e-02,  5.8766e-02,  2.1751e-01,\n",
      "         -1.8269e-01, -7.2737e-02, -8.5230e-02, -1.2250e-02,  2.8531e-02,\n",
      "         -2.5725e-02, -6.7673e-02,  1.5742e-02,  1.8245e-01, -6.8855e-02,\n",
      "         -2.4382e-02, -1.8381e-01, -1.6422e-01, -3.1009e-02, -1.3451e-01,\n",
      "         -2.5125e-02, -5.0697e-02, -6.2852e-02,  1.5068e-01, -8.7944e-02,\n",
      "          9.3806e-02, -3.9129e-02,  3.2089e-03, -2.8383e-02,  4.5505e-02,\n",
      "         -8.4022e-02, -9.5500e-02,  1.6939e-01,  2.2807e-01,  1.0754e-01,\n",
      "          9.6213e-02,  1.5700e-01,  1.7333e-01, -1.0886e-01],\n",
      "        [-1.0459e-01,  3.5434e-02,  1.5804e-01, -1.0998e-01, -1.0341e-01,\n",
      "         -1.1333e-01, -2.5810e-02, -2.0710e-01,  1.2931e-02,  5.9139e-02,\n",
      "          4.4665e-02,  1.6872e-02, -2.0280e-01, -6.1545e-02, -2.4018e-02,\n",
      "         -1.1853e-01, -4.9591e-03,  5.0667e-03,  1.6890e-01,  6.7350e-02,\n",
      "         -1.7759e-01, -1.2918e-02, -8.3269e-02, -1.4798e-01, -1.3551e-01,\n",
      "          7.9069e-02,  7.5186e-02,  4.8665e-02, -9.1255e-03, -1.3598e-01,\n",
      "         -1.0354e-01, -5.6978e-02, -4.7404e-02, -2.0780e-01,  3.2732e-01,\n",
      "         -8.6352e-03,  5.9043e-02,  7.6749e-02, -1.6786e-01,  1.7965e-02,\n",
      "         -6.7248e-02, -2.6245e-01, -1.0560e-01, -7.5362e-02, -4.0772e-02,\n",
      "         -1.3574e-02, -3.3656e-02,  1.3126e-01, -2.5366e-02, -8.6758e-02,\n",
      "         -2.3894e-02, -7.0907e-03,  1.8360e-01,  4.3601e-02,  6.8113e-02,\n",
      "         -1.5968e-01, -9.5084e-02, -1.5277e-01,  1.2606e-04,  1.0093e-01,\n",
      "          9.1947e-02,  5.3390e-02, -3.4258e-02, -4.0064e-02, -1.3629e-01,\n",
      "         -1.9423e-01, -9.2343e-02,  5.4306e-02,  6.8781e-02,  1.6131e-01,\n",
      "          1.0243e-01,  2.1735e-02,  1.6789e-01, -1.6689e-02,  1.9349e-01,\n",
      "          1.9063e-02,  3.3864e-02, -8.2810e-02,  7.5136e-02, -1.7725e-02,\n",
      "         -3.0833e-02, -9.8592e-02, -1.5412e-01, -5.5047e-02],\n",
      "        [ 5.9973e-02,  1.0144e-01,  2.0604e-02, -3.2352e-02,  8.5650e-02,\n",
      "          8.5927e-02,  2.4671e-02,  1.4224e-01,  1.2132e-01, -1.5860e-01,\n",
      "          3.0934e-02, -1.2973e-01,  6.5712e-02, -5.7191e-02,  2.0141e-01,\n",
      "         -1.9216e-01, -3.9727e-02, -1.1875e-01, -2.2891e-01,  2.9007e-01,\n",
      "          9.7442e-02,  8.9075e-02,  7.8504e-02,  1.3218e-02, -6.2382e-02,\n",
      "          6.0922e-02,  8.1496e-02,  2.1128e-01, -3.8415e-02, -1.1241e-01,\n",
      "         -8.6253e-03, -1.2514e-01, -1.8576e-01, -5.4394e-02,  1.4732e-01,\n",
      "          6.0452e-02, -4.9426e-02, -1.1490e-01, -2.4699e-02, -1.3939e-01,\n",
      "          8.7735e-02,  2.9694e-02,  1.7434e-01,  8.4930e-02,  1.6544e-01,\n",
      "          5.0141e-02, -5.7561e-02, -5.7130e-02, -4.4285e-02,  6.6936e-02,\n",
      "         -1.8040e-01, -2.3493e-01, -2.1801e-03,  2.7469e-01,  8.0981e-03,\n",
      "         -1.8161e-02, -9.0486e-02,  2.3680e-02,  2.7297e-02,  4.0449e-02,\n",
      "          8.7701e-03, -1.4754e-01, -7.4901e-02,  4.3841e-03,  1.1439e-01,\n",
      "          4.7792e-02, -6.6700e-02,  3.9740e-02,  4.3834e-03, -7.5861e-03,\n",
      "          1.0025e-01,  2.1714e-01,  5.8666e-02,  1.6728e-01,  3.7199e-02,\n",
      "         -7.7784e-02,  4.5659e-02, -5.3316e-02,  7.5027e-02, -9.4730e-02,\n",
      "         -7.5242e-02,  5.5339e-02,  3.4693e-02,  6.9589e-02],\n",
      "        [-8.1550e-02,  1.6084e-01,  8.3536e-02, -5.5337e-02,  1.2182e-01,\n",
      "          2.5518e-01,  2.4211e-02, -2.0292e-01, -2.5211e-02, -1.6989e-02,\n",
      "          1.1196e-01, -4.6854e-02, -4.6249e-02, -6.2325e-02,  1.4747e-01,\n",
      "          5.0202e-02,  5.0802e-02,  4.6495e-02, -7.6109e-02,  8.5610e-02,\n",
      "         -1.3976e-01,  1.7903e-01,  2.9298e-02,  1.6914e-01,  1.8043e-03,\n",
      "          7.4919e-02,  4.5543e-03, -1.5459e-01, -1.2929e-01, -1.0693e-01,\n",
      "         -1.3363e-01,  2.0650e-01,  3.9132e-02, -1.5141e-01, -1.1126e-01,\n",
      "          3.7581e-02,  1.9683e-02, -1.4754e-02,  2.8031e-02, -5.0378e-02,\n",
      "         -1.4196e-02,  8.0584e-02,  2.2039e-02,  5.3625e-02, -1.2013e-01,\n",
      "         -9.2710e-02, -1.0451e-01,  4.1376e-02, -1.4315e-01,  4.7168e-02,\n",
      "         -2.7055e-02, -9.6741e-02, -1.4681e-01,  9.2447e-02, -1.0078e-01,\n",
      "         -2.7550e-01,  1.5331e-01,  1.2511e-01, -1.2257e-01, -1.0139e-01,\n",
      "          8.6650e-02,  1.1764e-01,  2.3255e-02,  2.6762e-02, -5.4521e-02,\n",
      "          1.5801e-01, -4.4418e-03,  3.9369e-02,  7.5054e-02,  1.1951e-01,\n",
      "          4.3214e-02,  1.4164e-02, -5.0655e-02, -2.2348e-01, -1.9337e-01,\n",
      "          8.6477e-02, -1.0877e-01,  1.5859e-01,  1.0484e-02,  3.2034e-02,\n",
      "         -1.1987e-01, -9.7722e-02, -1.7904e-01, -7.6053e-02]],\n",
      "       requires_grad=True)\n",
      "fc3.bias Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import init\n",
    "\n",
    "class MyCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = torch.nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "        self.apply(my_custom_weight_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def my_custom_weight_init(tensor):\n",
    "    if isinstance(tensor, torch.nn.Linear):\n",
    "        torch.nn.init.orthogonal_(tensor.weight.data)\n",
    "        tensor.bias.data.fill_(1.0)\n",
    "    elif isinstance(tensor, torch.nn.Conv2d):\n",
    "        tensor.weight.data.fill_(1.0)\n",
    "        tensor.bias.data.fill_(1.0)\n",
    "\n",
    "model = MyCNN()\n",
    "#model.apply(my_custom_weight_init)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    #print(name, param.data.size())\n",
    "    print(name, param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "print(a[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Initialization of Lazy Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv output size:  torch.Size([32, 10, 10])\n",
      "fc_input size: 3200\n",
      "input image size: torch.Size([5, 3, 100, 100])\n",
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "\n",
    "def get_output_shape(model, image_dim):\n",
    "    return model(torch.rand(*(image_dim))).data.shape\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "    obs_shape,\n",
    "    feature_dim,\n",
    "    conv_layers = [64, 32, 32],\n",
    "    dense_layers = [1028, 512, 256]) -> None:\n",
    "        super().__init__()\n",
    "        assert len(obs_shape) == 3, \"image observation of shape (c, w, h) is expected\"\n",
    "        self.obs_shape = obs_shape\n",
    "        self.feature_dim = feature_dim\n",
    "        self.conv_layers = conv_layers \n",
    "        self.dense_layers = dense_layers \n",
    "        self.conv_out_shape = None \n",
    "\n",
    "\n",
    "        self.convs = nn.Sequential()\n",
    "        for i in range(len(self.conv_layers)):\n",
    "            if i == 0:\n",
    "                self.convs.append(nn.Conv2d(self.obs_shape[0], self.conv_layers[i], \n",
    "                                                 kernel_size=3, stride=1))\n",
    "            else:\n",
    "                self.convs.append(nn.Conv2d(self.conv_layers[i-1], self.conv_layers[i],\n",
    "                                                 kernel_size=3, stride=1))\n",
    "            self.convs.append(nn.ReLU())\n",
    "            self.convs.append(nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        \n",
    "        conv_out = get_output_shape(self.convs, self.obs_shape)\n",
    "        fc_input = np.prod(list(conv_out))\n",
    "        print('Conv output size: ', conv_out)\n",
    "        print('fc_input size:', fc_input)\n",
    "\n",
    "        self.fcs = nn.Sequential()\n",
    "        for i in range(len(self.dense_layers)):\n",
    "            if i == 0:\n",
    "                #self.fcs.append(nn.LazyLinear(self.dense_layers[i]))\n",
    "                self.fcs.append(nn.Linear(fc_input, self.dense_layers[i]))\n",
    "            else:\n",
    "                self.fcs.append(nn.Linear(self.dense_layers[i-1], self.dense_layers[i]))\n",
    "            self.fcs.append(nn.ReLU())\n",
    "        self.fcs.append(nn.Linear(self.dense_layers[-1], self.feature_dim))\n",
    "\n",
    "        \n",
    "    def forward(self, obs, detach=False):\n",
    "        obs = obs / 255.0\n",
    "\n",
    "        out = self.convs(obs)\n",
    "        out = torch.flatten(out, start_dim=1)  # flatten starting from dim = 1\n",
    "        out = self.fcs(out)\n",
    "\n",
    "        if detach:\n",
    "            out = out.detach()\n",
    "        return out\n",
    "\n",
    "obs_shape = (3, 100, 100)  # (C, H, W)\n",
    "enc = Encoder(obs_shape, feature_dim=10)\n",
    "\n",
    "input_img = torch.rand(5, 3, 100, 100)\n",
    "print('input image size:', input_img.size())\n",
    "output = enc(input_img)\n",
    "\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, 2, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "x = torch.rand((1, 3, 32, 32))\n",
    "autoenc = Autoencoder()\n",
    "x_recon = autoenc(x) \n",
    "\n",
    "print(x_recon.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two tuples do not match at index 2.\n"
     ]
    }
   ],
   "source": [
    "def assert_match_tuples(tuple1, tuple2):\n",
    "    \"\"\"Assert that two tuples match.\n",
    "\n",
    "    Args:\n",
    "        tuple1 (tuple): The first tuple.\n",
    "        tuple2 (tuple): The second tuple.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the two tuples do not match.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(tuple1) == len(tuple2), \"The two tuples must have the same length.\"\n",
    "    for i in range(len(tuple1)):\n",
    "        assert tuple1[i] == tuple2[i], \"The two tuples do not match at index {}.\".format(i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tuple1 = (1, 2, 3)\n",
    "    tuple2 = (1, 2, 3)\n",
    "    assert_match_tuples(tuple1, tuple2)\n",
    "\n",
    "    tuple1 = (1, 2, 3)\n",
    "    tuple2 = (1, 2, 4)\n",
    "    try:\n",
    "        assert_match_tuples(tuple1, tuple2)\n",
    "    except AssertionError as e:\n",
    "        print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
